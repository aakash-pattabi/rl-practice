In this project, I'll be implementing a series of classic reinforcement learning (RL) algorithms, simply as a personal project to improve my own competencies in RL. I'll mostly follow the reading list from [this Reddit post](https://old.reddit.com/r/reinforcementlearning/comments/8k356e/new_phd_student_what_papers_should_i_read_first/), implementing classical control/Markov Decision Process (MDP) solver algorithms such as value iteration as well as more modern RL algorithms. 

Additionally, I'll try to discuss each paper that I read, summarize it as best as I can, and dig in, in some sense, to the intuition underlying the math. 

In order: 

	* [ ] [Temporal difference learning (Sutton 1998)](https://link.springer.com/content/pdf/10.1007/BF00115009.pdf)

	* [ ] [Q-learning (Watkins and Dayan 1992)](https://link.springer.com/content/pdf/10.1007/BF00992698.pdf)
